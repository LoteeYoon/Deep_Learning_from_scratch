{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 자연어 처리란"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 자연어 처리 분야의 본질적 문제는 컴퓨터가 인간의 언어를 이해하게 만드는 것입니다.\n",
    "## 자연어(Natural Language)는 한국어와 영어 등 우리가 평소에 쓰는 말을 의미합니다.\n",
    "## 단어는 의미의 최소 단위이므로 자연어를 컴퓨터에게 이해시키는 데는 '단어의 의미'를 이해시키는 것이 핵심입니다.\n",
    "\n",
    "\n",
    "## 단어의 의미를 잘 파악하는 표현 방법의 세 가지 기법\n",
    "## 1. 시소러스를 활용한 기법\n",
    "## 2. 통계 기반 기법\n",
    "## 3. 추론 기반 기법(word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 시소러스(thesaurus) - 유의어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어의 의미를 나타내는 방법으로 먼저 사람이 직접 단어의 의미를 정의하는 방식을 생각할 수 있다.\n",
    "## '뜻이 같은 단어(동의어)', 또는 '뜻이 비슷한 단어(유의어)' 들이 그룹을 이루어 분류되어 있다.\n",
    "## 또 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등 더 세세한 관계까지 정의한 경우도 존재\n",
    "\n",
    "## 이처럼 모든 단어에 대한 유의어 집합을 만든 다음, 단어들의 관계를 그래프로 표현하여 단어 사이의 연결을 정의할 수 있음\n",
    "## 그래서 이 단어 네트워크를 이용해 컴퓨터에게 단어 사이 관계를 가르칠 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 자연어 처리 분야에서 가장 유명한 시소러스로는 WordNet이 있다.\n",
    "\n",
    "## WordNet을 이용하면 유의어를 얻거나 단어 네크워크를 사용해 단어 사이의 유사도를 구할 수 있다.\n",
    "\n",
    "## 그러나 시소러스에는 문제점이 있다.\n",
    "## 1. 시대 변화에 대응하기 어렵다 - 시대 변화에 따른 단어 변화에 대응하기 위해선 수작업으로 끊임없이 갱신해야함\n",
    "## 2. 사람을 쓰는 비용은 크다 - 시소러스를 만드는 데는 엄청난 인적 비용이 발생한다.\n",
    "## 3. 단어의 미묘한 차이를 표현할 수 없다.\n",
    "\n",
    "## 이러한 문제를 해결하기 위해 '통계 기반 기법'과 '추론 기반 기법'이 등장했다.\n",
    "## 이 두 기법은 대량의 텍스트 데이터로부터 단어의 의미를 자동으로 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 통계 기반 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 통계 기반 기법을 살펴보면서 말뭉치(Corpus)를 이용할 것이다.\n",
    "## 말뭉치란 대량의 텍스트 데이터를 의미한다.\n",
    "## 다만 자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터를 말한다.\n",
    "\n",
    "## 이러한 말뭉치에는 문장을 쓰는 방법, 단어를 선택하는 방법, 단어의 의미 등 사람이 알고 있는 자연어에 대한 지식이\n",
    "## 포함되어 있고 이러한 말뭉치에서 자동으로 그 핵심을 추출하는 것이 통계 기반 기법의 목표다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 자연어 처리에는 다양한 말뭉치가 사용된다\n",
    "## 위키백과, 구글뉴스 등의 텍스트 데이터를 예로 들 수 있다. 또 소설 작품들도 이용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이썬으로 말뭉치 전처리하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 말뭉치 전처리란 텍스트 데이터를 단어로 분할하고 분할된 단어들을 '단어 ID 목록'으로 변환하는 일이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'You say goodbye and I say hello.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you say goodbye and i say hello  .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower() ## 소문자로 변환\n",
    "text = text.replace('.', ' .')  ## 문장 끝 마침표를 공백 + 마침표로 변환\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'say', 'goodbye', 'and', 'i', 'say', 'hello', '', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ') ## 공백을 기준으로 단어를 분할\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이제 문장을 단어 목록 형태로 이용할 수 있게 되었다.\n",
    "## 단어를 텍스트 그대로 조작하기 어려우므로 단어에 ID를 부여하고 리스트로 이용할 수 있도록 한 번 더 손질한다.\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '', 7: '.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '': 6, '.': 7}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이처럼 딕셔너리를 이용하면 단어를 가지고 ID를 검색하거나 반대로 ID를 가지고 단어를 검색할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 마지막으로 단어 목록을 단어 ID 목록으로 변경하고 넘파이 배열로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 1, 5, 6, 7])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corpus = [word_to_id[w] for w in words] ## 파이썬의 내포(comprehension) 표기\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "            \n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이 함수를 사용하면 말뭉치 전처리를 다음과 같이 수행할 수 있다.\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## corpus는 단어 ID 목록\n",
    "## word_to_id는 단어에서 단어 ID로의 딕셔너리\n",
    "## id_to_word는 단어 ID에서 단어로의 딕셔너리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어의 분산 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 색을 (R, G, B) = (170, 33, 22)처럼 벡터로 표현할 수 있듯이 단어 또한 벡터로 표현 가능하다.\n",
    "## 단어의 분산표현(distributional representation)은 단어의 의미를 정확히 파악할 수 있는 벡터 표현이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분포 가설"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 자연어 처리 역사에서 단어를 벡터로 표현하는 연구는 수없이 이루어졌다.\n",
    "## 그 연구들은 단 하나의 간단한 아이디어에 뿌리를 두고 있다.\n",
    "## 그 아이디어는'단어의 의미는 주변 단어에 의해 형성된다'는 분포가설(distributional hypothesis)이며 최근 연구도 대부분\n",
    "## 이 가설에 기초한다.\n",
    "\n",
    "## 분포가설에서 단어 자체는 의미가 없고, 그 단어가 사용된 '맥락(context)'이 의미를 형성한다고 한다.\n",
    "## 예를 들면 i drink beer 와 we drink wine처럼 drink 주변에는 음료가 등장하기 쉽다.\n",
    "## 맥락이라는 표현은 주목하는 단어 주변에 놓인 단어를 의미한다.\n",
    "## 맥락의 크기를 '윈도우 크기(window size)'라고 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동시발생 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이제 분포 가설에 기초해 단어를 벡터로 나타내는 방법을 생각해보자\n",
    "\n",
    "## 어떤 단어에 주목했을 때, 그 주변에 특정 단어가 몇 번이나 등장하는지를 세는 방법이 있는데 이를 '통계 기반 기법'이라고 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 여기서 단어의 수가 총 7개임을 알 수 있다\n",
    "## 다음으로 맥락에 해당하는 단어의 빈도를 세어보자\n",
    "## 윈도우 크기는 1로 하고, 단어 ID가 0인 'you'부터 시작해보자\n",
    "\n",
    "## you의 맥락은 say라는 단어 하나뿐이다.\n",
    "## 이를 바탕으로 you라는 단어를 [0, 1, 0, 0, 0, 0, 0]이라는 벡터로 표현할 수 있다.\n",
    "## 계속해서 모든 단어에 대해 같은 작업을 반복한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모든 단어에 대해 작업을 수행한 결과\n",
    "\n",
    "## you     [0, 1, 0, 0, 0, 0, 0]\n",
    "## say     [1, 0, 1, 0, 1, 1, 0]\n",
    "## goodbye [0, 1, 0, 1, 0, 0, 0]\n",
    "## and     [0, 0, 1, 0, 1, 0, 0]\n",
    "## i       [0, 1, 0, 1, 0, 0, 0]\n",
    "## hello   [0, 1, 0, 0, 0, 0, 1]\n",
    "## .       [0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "## 이 표의 각 행은 해당 단어를 표현한 벡터가 된다.\n",
    "## 이 표가 행렬의 형태를 띤다는 뜻에서 동시발생 행렬(co-occurrence matrix)라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "], dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n",
      "[0 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) ## ID가 0인 단어의 벡터 표현\n",
    "print(C[4]) ## ID가 4인 단어의 벡터 표현\n",
    "print(C[word_to_id['goodbye']]) ## 'goodbye'의 벡터 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이와 같이 동시발생 행렬을 활용하면 단어를 벡터로 나타낼 수 있다.\n",
    "## 지금까지 수동으로 동시발생 행렬을 만들었지만 자동화 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size = 1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype = np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "            \n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "                \n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위의 함수는 말뭉치가 아무리 커지더라도 자동으로 동시발생 행렬을 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터 간 유사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 앞에서 동시발생 행렬을 활용해 단어를 벡터로 표현하는 방법을 알아봤습니다.\n",
    "## 이제 벡터 사이의 유사도를 측정하는 방법을 살펴보자.\n",
    "\n",
    "## 벡터 사이의 유사도를 측정하는 방법은 '벡터의 내적'이나 '유클리드 거리' 등을 예로 들 수 있다.\n",
    "## 그 외 다양하게 있지만, 단어 벡터 유사도를 나타낼 때는 '코사인 유사도(cosine similarity)'를 자주 이용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## similarity(x, y) = inner product(x, y) / (L2 Norm(x) * L2 Norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 코사인 유사도를 직관적으로 보면 두 벡터가 가리키는 방향이 얼마나 비슷한가입니다.\n",
    "## 두 벡터 방향이 완전히 같다면 유사도가 1이 되며 반대면 -1이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y):\n",
    "    nx = x / np.sqrt(np.sum(x**2)) # x의 정규화\n",
    "    ny = y / np.sqrt(np.sum(y**2)) # y의 정규화\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 제로 벡터가 들어오면 생기는 문제 해결\n",
    "\n",
    "def cos_similarity(x, y, eps = 1e-8):\n",
    "    nx = x / (np.sqrt(np.sum(x**2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y**2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067691154799\n"
     ]
    }
   ],
   "source": [
    "## 'you'와 'i'의 유사도를 구하는 코드\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 유사도가 0.7이므로 유사성이 있다고 말 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유사 단어의 랭킹 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 어떤 단어가 검색어로 주어지면 그 검색어와 비슷한 단어를 유사도 순으로 출력하는 함수를 구현해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top = 5):\n",
    "    # 검색어를 꺼낸다\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query]' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "        \n",
    "    # 유사도를 기준으로 내림차순 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort(): # argsort() = 가장 작은 값부터 순서대로 index 반환\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], simiarity[i]))\n",
    "        \n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query] you\n",
      " goodbye: 0.7071067691154799\n",
      " i: 0.7071067691154799\n",
      " hello: 0.7071067691154799\n",
      " say: 0.0\n",
      " and: 0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess, create_co_matrix, most_similar\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "\n",
    "most_similar('you', word_to_id, id_to_word, C, top = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 통계 기반 기법 개선하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상호정보량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 동시발생 행렬의 원소는 두 단어가 동시에 발생한 횟수를 나타냅니다. 그러나 이 발생 횟수라는 것은 사실 좋은 특징이 아닙니다.\n",
    "## the car와 같은 두 단어의 경우 동시 발생 횟수가 아주 많을 것입니다.\n",
    "## 한 편 car와 drive는 확실히 관련성이 높은데 횟수만을 본다면 the가 강한 관련성을 가질 것입니다.\n",
    "## 이 문제를 해결하기 위해 점별 상호정보량(Pointwise Mutual Information, PMI)이라는 척도를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PMI는 확률 변수 x와 y에 대해 다음 식으로 정의된다\n",
    "## PMI(x, y) = log2(P(x, y) / P(x) * P(y)) ----- P는 확률변수가 일어날 확률을 의미\n",
    "## 이 PMI 값이 높을수록 관련성이 높다는 것을 의미한다.\n",
    "\n",
    "## ex) 10000개의 단어로 이루어진 말뭉치에서 the가 100번 등장하면 P(the) = 100 / 10000 = 0.01\n",
    "\n",
    "## 동시발생 행렬을 사용하여 PMI 식을 나타낼 수 있다.\n",
    "## C(x, y)는 x와 y의 동시발생 횟수를 의미한다.\n",
    "## PMI(x, y) = log2((C(x,y)/N) / (C(x)/N) * (C(y)/N)) = log2(C(x, y) * N / C(x) * C(y))\n",
    "## 따라서 동시발생 행렬로부터 PMI를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ex) 말뭉치 단어 수(N) = 10000, the = 1000, car = 20, drive = 10 \n",
    "## the와 car의 동시발생 횟수는 10회, car와 drive의 동시발생 횟수는 5회라고 할 때, \n",
    "## PMI('the', 'car') = log2(10 * 10000 / 1000 * 20) = 2.32\n",
    "## PMI('car', 'drive') = log2(5 * 10000 / 20 * 10) = 7.97\n",
    "\n",
    "## PMI는 단어가 단독으로 출현하는 횟수를 고려했기 때문에 좀 더 관련성을 잘 나타낼 수 있다.\n",
    "\n",
    "\n",
    "## PMI는 두 단어의 동시발생 횟수가 0이면 log2(0) = -Inf 가 된다. 이 문제를 피하기 위해 실제로 구현에서는\n",
    "## Positive PMI(PPMI)를 사용합니다. PPMI(x, y) = max(0, PMI(x, y))\n",
    "## 이 식에 따라 PMI가 음수일 때는 0으로 취급합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose = False, eps = 1e-8):\n",
    "    M = np.zeros_like(C, dtype = np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis = 0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total // 100) == 0:\n",
    "                    print('%.1f%% 완료' % (100 * cnt / total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생 행렬\n",
      "[[0 1 0 0 0 0 0]\n",
      " [1 0 1 0 1 1 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 0 1 0 1 0 0]\n",
      " [0 1 0 1 0 0 0]\n",
      " [0 1 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      " [1.807 0.    0.807 0.    0.807 0.807 0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.    1.807 0.    1.807 0.    0.   ]\n",
      " [0.    0.807 0.    1.807 0.    0.    0.   ]\n",
      " [0.    0.807 0.    0.    0.    0.    2.807]\n",
      " [0.    0.    0.    0.    0.    2.807 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.util import preprocess, create_co_matrix, cos_similarity, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision = 3) ## 유효자릿수를 세 자리로 표시\n",
    "print('동시발생 행렬')\n",
    "print(C)\n",
    "print('-' * 50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 차원 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PPMI 행렬의 문제점은 말뭉치의 어휘 수가 증가함에 따라 각 단어 벡터의 차원 수도 증가한다는 것이다.\n",
    "## 그리고 대부분의 행렬이 0인 것을 알 수 있다. 이는 원소 대부분이 중요하지 않다는 뜻이다.\n",
    "## 이런 문제에 대처하고자 사용하는 기법이 차원 감소이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 차원 축소는 벡터의 차원을 줄이는 방법이지만 단순히 줄이는 것이 아니라 중요한 정보는 최대한 유지하면서 줄이는 게 핵심입니다.\n",
    "## 원소 대부분이 0인 행렬 또는 벡터를 희소행렬(sparse matrix), 희소벡터(sparse vector)라고 한다. \n",
    "## 차원 축소의 핵심은 희소벡터에서 중요한 축을 찾아내어 더 적은 차원으로 다시 표현하는 것인데, 차원 감소의 결과로 원래의 \n",
    "## 희소벡터는 원소 대부분이 0이 아닌 값으로 구성된 '밀집벡터'로 변환된다.\n",
    "## 이 밀집벡터가 우리가 원하는 단어의 분산표현이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 차원 축소의 방법은 여러 가지가 있는데, 여기선 특잇값분해(singular value decomposition, SVD)를 사용한다.\n",
    "## SVD는 임의의 행렬 X를 U, S, V라는 세 행렬의 곱으로 분해한다. U와 V는 직교행렬(orthogonal matrix)이고\n",
    "## S는 대각행렬(diagonal matrix)이다.\n",
    "\n",
    "## 직교행렬 -- 행벡터와 열벡터가 유클리드 공간의 정규직교기저를 이루는 실수 행렬\n",
    "## 정규직교기저란? 벡터의 크기가 1이고 서로 수직인 기저 벡터를 의미, 역행렬과 전치행렬이 동일하다.\n",
    "\n",
    "## 대각행렬 -- 대각성분 외에는 모두 0인 행렬을 의미\n",
    "\n",
    "## U는 어떠한 공간의 축(기저)을 형성한다. 여기서 우리는 단어 공간으로 취급할 수 있다\n",
    "## S의 대각성분들은 특잇값이 큰 순서로 나열되어 있다. 특잇값이란 해당 축의 중요도라고 간주할 수 있다.\n",
    "## 따라서 SVD는 중요도가 낮은 원소를 깎아 내는 방법이라 볼 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD에 의한 차원 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from common.util import preprocess, create_co_matrix, ppmi\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(id_to_word)\n",
    "C = create_co_matrix(corpus, vocab_size, window_size = 1)\n",
    "W = ppmi(C)\n",
    "\n",
    "U, S, V = np.linalg.svd(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n",
      "[0.    1.807 0.    0.    0.    0.    0.   ]\n",
      "[ 3.409e-01 -1.110e-16 -3.886e-16 -1.205e-01 -9.323e-01  0.000e+00\n",
      "  1.958e-17]\n"
     ]
    }
   ],
   "source": [
    "print(C[0]) # 동시발생 행렬\n",
    "print(W[0]) # PPMI 행렬\n",
    "print(U[0]) # SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.409e-01 -1.110e-16]\n"
     ]
    }
   ],
   "source": [
    "## 이 밀집 벡터 U를 2차원으로 줄이려면 단순히 처음 두 원소를 꺼내면 된다\n",
    "print(U[0, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 행렬의 크기가 N이면 SVD 계산은 O(N3)이 걸린다. N의 세제곱에 비례해 늘어난다. 그래서 작은 특잇값은 버리는 Truncated SVD을\n",
    "## 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PTB 데이터셋 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PTB = Penn Treebank\n",
    "## PTB 말뭉치는 주어진 기법의 품질을 측정하는 벤치마크로 주로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정리\n",
    "\n",
    "## 마침내 '단어의 의미'를 벡터로 인코딩하는 데 성공했다.\n",
    "## 말뭉치를 이용해 맥락에 속한 단어의 등장 횟수를 센 후 PPMI 행렬로 변환하고, 다시 SVD를 이용해 차원을 감소시킴으로써\n",
    "## 더 좋은 단어 벡터를 얻었다. 이것이 단어의 분산표현이고 각 단어는 고정 길이의 밀집벡터로 표현되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
